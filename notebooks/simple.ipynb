{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Optimiization with PyTorch\n",
    "\n",
    "The aim of this notebbok is to try to ouptimize simple functions using PyTorch and gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "is_cuda = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First function\n",
    "\n",
    "The first function will be a one variable function of the form :\n",
    "\n",
    "$$\n",
    "f(x) = 0.01x^4 + 1.5x^3 + 23x^2 - 10x - 1 - 4x\\*e^{-x-200}\n",
    "$$\n",
    "\n",
    "We will first plot the function and then optimize it using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-200, 100, 0.05)\n",
    "y = 0.01 * x ** 4 + 1.5 * x ** 3 + 23 * x ** 2 + 10 * x - 1 - 4 * x * np.exp(-x - 200)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (\n",
    "        0.01 * x ** 4\n",
    "        + 1.5 * x ** 3\n",
    "        + 23 * x ** 2\n",
    "        + 10 * x\n",
    "        - 1\n",
    "        - 4 * x * torch.exp(-x - 200)\n",
    "    )\n",
    "\n",
    "\n",
    "def f_numpy(x):\n",
    "    return (\n",
    "        0.01 * x ** 4\n",
    "        + 1.5 * x ** 3\n",
    "        + 23 * x ** 2\n",
    "        + 10 * x\n",
    "        - 1\n",
    "        - 4 * x * np.exp(-x - 200)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.tensor([2], dtype=torch.float32, requires_grad=True)\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.SGD([x0], lr=learning_rate)\n",
    "\n",
    "for i, epoch in enumerate(range(n_epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    out = f(x0)\n",
    "    out.backward()\n",
    "    if i % 100 == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch}: x0 = {x0.item()}, f(x0) = {out.item()}, gradient = {x0.grad.item()}\"\n",
    "        )\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deuxième fonction plus simple pour vérifier\n",
    "\n",
    "La deuxième fonction peut s'optimiser à la main afin de vérifier que le code est correct.\n",
    "\n",
    "\\\\begin{align\\*}\n",
    "g(x) &= 3x^2 + 6x +1 \\\\\n",
    "g'(x) &= 6x + 6\n",
    "\\\\end{align\\*}\n",
    "\n",
    "On voit bien que $x\\_{min} = -1$ et $f(x\\_{min}) = -2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return 3 * x ** 2 + 6 * x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.tensor([5], dtype=torch.float32, requires_grad=True)\n",
    "n_epochs = 50\n",
    "display_rate = 5\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD([x0], lr=learning_rate)\n",
    "\n",
    "for i, epoch in enumerate(range(n_epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    out = g(x0)\n",
    "    out.backward()\n",
    "    if i % display_rate == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch}: x0 = {x0.item()}, f(x0) = {out.item()}, gradient = {x0.grad.item()}\"\n",
    "        )\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que la méthode converge vite vers la valeur minimale, bien qu'il y est quand même une petite différence.\n",
    "\n",
    "Essayons avec l'optimizer Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.tensor([5], dtype=torch.float32, requires_grad=True)\n",
    "n_epochs = 200\n",
    "display_rate = 20\n",
    "learning_rate = 0.3\n",
    "optimizer = torch.optim.Adam([x0], lr=learning_rate)\n",
    "\n",
    "for i, epoch in enumerate(range(n_epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    out = g(x0)\n",
    "    out.backward()\n",
    "    if i % display_rate == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch}: x0 = {x0.item()}, f(x0) = {out.item()}, gradient = {x0.grad.item()}\"\n",
    "        )\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction à plusieurs variables\n",
    "\n",
    "Dans cette partie essayons de trouver les valeurs optimales pour une fonction à plusieurs variables.\n",
    "\n",
    "$$\n",
    "h(x, y) = x^2 + y^2 + xy + x + y + 1\n",
    "$$\n",
    "\n",
    "La solution est $x=-\\\\frac{1}{3}$ et $y=-\\\\frac{1}{3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x, y):\n",
    "    return x ** 2 + y ** 2 + x * y + x + y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.tensor([5], dtype=torch.float32, requires_grad=True)\n",
    "y0 = torch.tensor([5], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "n_epochs = 1001\n",
    "display_rate = 100\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam([x0, y0], lr=learning_rate)\n",
    "\n",
    "for i, epoch in enumerate(range(n_epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    out = h(x0, y0)\n",
    "    out.backward()\n",
    "    if i % display_rate == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch}: x0 = {x0.item()}, y0 = {y0.item()}, f(x0, y0) = {out.item()}, gradient = {x0.grad.item()}, {y0.grad.item()}\"\n",
    "        )\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eec538e602f9557dc9662e6bc681b9823109cc6a965a6e8a365c0a2dd6a2372e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
